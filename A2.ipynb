{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VVGc4hLOG2Ty"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ribesstefano/chalmers_dat450_ml_for_nlp/blob/main/A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook could be found at this link: https://colab.research.google.com/drive/1f2nRQyfELRy0H8q0i5dFrVinJeuiTvz9?usp=sharing\n",
        "\n",
        "Group N - Authors:\n",
        "\n",
        "* Stefano Ribes\n",
        "* Cody Hesse\n",
        "* Apoorva x"
      ],
      "metadata": {
        "id": "Z92Kf5zRdAlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: Topic Modeling"
      ],
      "metadata": {
        "id": "lbTppIpgdTuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Imports"
      ],
      "metadata": {
        "id": "G8Os8hyYdZF4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6x-MnPJpJGJ",
        "outputId": "17ac737e-21d2-4dc5-9755-f33a5c387421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('reuters')\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import reuters, brown, stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus Download and Cleaning"
      ],
      "metadata": {
        "id": "EceHbBh5db4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text, lowercase=True):\n",
        "    if lowercase:\n",
        "        return [t.text.lower() for t in nlp.tokenizer(text)]\n",
        "    else:\n",
        "        return [t.text for t in nlp.tokenizer(text)]\n",
        "\n",
        "def process_corpus(corpus, stop_words: set) -> list:\n",
        "    \"\"\"Remove capitalizations and stop-words from raw_df, store in long-format DataFrame\"\"\"\n",
        "    token_doc_tups = []\n",
        "    for document in corpus:\n",
        "        tokens = tokenize(document.lower())\n",
        "        tokens_clean = [t.lower() for t in tokens if t.lower() not in stop_words and t.isalpha()]\n",
        "        token_doc_tups.append(tokens_clean)\n",
        "    # Just to be sure, remove extra escape characters and punctuations if still\n",
        "    # present in the words\n",
        "    token_doc_tups = [' '.join(w for w in line).rstrip('.,\\n').lower().split(' ') for line in token_doc_tups]\n",
        "    return token_doc_tups\n",
        "\n",
        "\n",
        "def clean_corpus(corpus_raw, freq_threshold=10):\n",
        "    # Load nltk stop-words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Loop corpus to find uncommon words (less than 10 occurances)\n",
        "    corpus_freqs = Counter(t.lower() for x in corpus_raw for t in tokenize(x) if t.isalpha())\n",
        "    uncommon = set([word for (word, freq) in corpus_freqs.items() if freq < freq_threshold])\n",
        "    # Update stop_words to include uncommon words\n",
        "    stop_words.update(uncommon)\n",
        "    # Tokenize corpus and get list of documents\n",
        "    cleaned_corpus = process_corpus(corpus_raw, stop_words)\n",
        "    return cleaned_corpus"
      ],
      "metadata": {
        "id": "GF47-5SOpTOt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select which corpus to analyze."
      ],
      "metadata": {
        "id": "dGmVdSuQCL8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_name = 'reuters'\n",
        "# corpus_name = '20newsgroups'"
      ],
      "metadata": {
        "id": "i3nejlSCB-LE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 20 News Group"
      ],
      "metadata": {
        "id": "nRLnOaXkCTt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),\n",
        "                shuffle=True, random_state=42)\n",
        "if corpus_name == '20newsgroups':\n",
        "    # Process Sklearn news corpus\n",
        "    cleaned_corpus = clean_corpus(newsgroups_test['data'], freq_threshold=10)"
      ],
      "metadata": {
        "id": "lRYkQRG_qqyr"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reuters"
      ],
      "metadata": {
        "id": "sfuE44yJCW_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_raw_df(corpus, n_files: int) -> pd.DataFrame:\n",
        "    \"\"\"Read raw text from corpus files and store in pandas DataFrame\"\"\"\n",
        "    raw_text = []\n",
        "    for file_id in corpus.fileids()[0:n_files]:\n",
        "        raw_text.append(corpus.raw(file_id))\n",
        "    return pd.DataFrame(raw_text, columns=['text'])\n",
        "if corpus_name == 'reuters':\n",
        "    # Process Reuters corpus\n",
        "    cleaned_corpus = clean_corpus(build_raw_df(reuters, n_files=4000)['text'])"
      ],
      "metadata": {
        "id": "6I2f8DR-BwL1"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpora = {\n",
        "    'reuters': clean_corpus(build_raw_df(reuters, n_files=4000)['text']),\n",
        "    '20newsgroups': clean_corpus(newsgroups_test['data'], freq_threshold=10),\n",
        "}"
      ],
      "metadata": {
        "id": "mm5h9vsCkajY"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaned Corpus Statistics"
      ],
      "metadata": {
        "id": "X2DV5AmaCepS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some statistics on the corpus size\n",
        "n_words = 0\n",
        "for document in cleaned_corpus:\n",
        "    n_words += len(document)\n",
        "print(f'Number of words in (tokenized) {corpus_name} corpus: {n_words}')"
      ],
      "metadata": {
        "id": "XDl7r1p_pVfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366cb1e0-3bbc-40d9-d2f5-3ae00c16d572"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in (tokenized) reuters corpus: 274516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(8):\n",
        "    print(f'Document n.{i}: {\" \".join(word for word in cleaned_corpus[i])}')"
      ],
      "metadata": {
        "id": "46zl5mKRpzLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0880fa45-c504-4217-8ec8-b1a29f3be69d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document n.0: asian exporters fear damage mounting trade friction japan raised fears among many asia exporting nations row could far reaching economic damage businessmen officials said told reuter asian move japan might boost protectionist sentiment lead curbs american imports products exporters said conflict would hurt long run short term tokyo loss might gain said impose mln dlrs tariffs imports japanese electronics goods april retaliation japan alleged failure pact sell semiconductors world markets cost japanese estimates put impact tariffs billion dlrs major electronics firms said would virtually halt exports products hit new taxes would able business said spokesman leading japanese electronics firm electric industrial co ltd tariffs remain place time beyond months mean complete exports goods subject tariffs said stock analyst tokyo office broker co taiwan businessmen officials also worried aware threat japan serves warning us said senior trade official asked named taiwan trade trade surplus billion dlrs last year pct surplus helped taiwan foreign exchange reserves billion dlrs among world largest must quickly open markets remove trade barriers cut import tariffs allow imports products want problems possible retaliation said paul chairman textile exporters group senior official south korea trade promotion association said trade dispute japan might also lead pressure south korea whose chief exports similar japan last year south korea trade surplus billion dlrs billion dlrs malaysia trade officers businessmen said tough curbs japan might allow hard hit producers semiconductors third countries expand sales hong kong newspapers alleged japan selling cost semiconductors electronics manufacturers share view businessmen said short term commercial advantage would pressure block imports short term view said mills director general federation hong kong industry whole purpose prevent imports one day extended sources much serious hong kong action trade said last year hong kong biggest export market accounting pct domestically produced exports australian government awaiting outcome trade talks japan interest concern industry minister john said last friday kind deterioration trade relations two countries major trading partners serious matter said said australia concerns coal beef australia two largest exports japan also significant exports country meanwhile diplomatic solve trade stand continue japan ruling liberal democratic party yesterday outlined package economic measures boost japanese economy measures proposed include large budget record public works spending first half financial year also call spending emergency measure stimulate economy despite prime minister yasuhiro nakasone fiscal reform program deputy trade representative michael smith japan deputy minister international trade industry miti due meet washington week effort end dispute\n",
            "Document n.1: china daily says pct grain stocks survey seven showed seven pct china grain stocks china daily said also said year mln tonnes pct china output left mln tonnes pct paper blamed waste inadequate storage bad methods said government launched national programme reduce waste calling improved technology storage greater production paper gave details\n",
            "Document n.2: japan revise long term energy demand ministry international trade industry miti revise long term energy supply demand outlook august meet forecast japanese energy demand ministry officials said miti expected lower primary energy supplies year mln mln said decision follows structural changes japanese industry following rise value yen decline domestic electric power demand miti planning work revised energy supply demand outlook committee meetings agency natural resources energy officials said said miti also review breakdown energy supply sources including oil nuclear coal natural gas nuclear energy provided bulk japan electric power fiscal year ended march estimated pct hour basis followed oil pct natural gas pct noted\n",
            "Document n.3: thai trade deficit first quarter thailand trade deficit widened billion first quarter billion year ago business economics department said said march imports rose billion billion thailand improved business climate year resulted pct increase imports raw materials semi finished products country oil import bill however fell pct first quarter due lower oil prices department said first quarter exports expanded billion billion export growth smaller expected due lower earnings many key commodities including rice whose earnings declined pct maize pct sugar pct tin pct seven pct products high export growth pct pct rubber pct\n",
            "Document n.4: indonesia sees price rising sharply indonesia expects crude palm oil prices rise sharply dlrs tonne fob year better european demand fall malaysian output minister crops told indonesian reporters prices malaysian around dlrs tonne cif delivery rotterdam traders said said indonesia would maintain exports despite making recent palm oil purchases malaysia could possibly increase international market share indonesia world second largest producer palm oil malaysia forced import palm oil ensure supplies month said better import cover temporary shortage lose export markets indonesian exports calendar tonnes according central bank figures\n",
            "Document n.5: australian foreign ship ban ends nsw ports hit new south nsw western australia yesterday lifted ban foreign flag ships carrying nsw ports still separate dispute shipping sources said ban imposed week ago pay claim movement port nearly vessels said pay dispute went hearing commission today meanwhile began today cargo handling ports sydney port said industrial action nsw ports part week action called nsw trades labour council protest changes state workers compensation laws shipping sources said various port unions appear taking turn work short time start shift cargo handling ports container movements affected stopped said said could say long go effect shipping movements\n",
            "Document n.6: indonesian commodity exchange may expand indonesian commodity exchange likely start trading least one new commodity possibly two calendar exchange chairman said told reuters telephone interview trading palm oil tobacco considered trading either crude palm oil refined palm oil may also introduced said question still considered trade minister decision go ahead made exchange currently trades coffee rubber open system four days week several factors make us move said want move slowly make confidence exchange physical rubber trading launched coffee added january rubber contracts traded fob five months forward coffee grades four five traded delivery five months forward exchange officials said trade ministry exchange board considering introduction futures trading later rubber one official said feasibility study needed first decisions likely indonesia elections april traders said trade minister said monday indonesia world second largest producer natural rubber expand rubber marketing effort hoped development exchange would help said exchange trying boost overseas interest building contacts end users said already south korea taiwan encourage direct use exchange delegation would also visit europe mexico latin american states encourage participation officials say exchange made good start although trading coffee transactions rubber start trading april december totalled tonnes worth mln dlrs fob plus mln rubber delivered latest exchange report said trading coffee calendar amounted tonnes valued billion total membership exchange nine brokers traders\n",
            "Document n.7: sri lanka gets usda approval wheat price food department officials said department agriculture approved continental grain co sale tonnes soft wheat dlrs tonne c f pacific northwest colombo said shipment april delivery\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA With Collapsed Gibbs Sampling"
      ],
      "metadata": {
        "id": "968urZcacDSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NumBa Acceleration\n",
        "\n",
        "We target two functions for acceleration:\n",
        "* `initialiaze`: given a document, _i.e._ a matrix of (`n_docs`, `n_words`) containing the indexes of words in the vocabulary, populates the topic and count matrixes\n",
        "* `sample`: given the aforementioned topic and count matrixes, update conditional distribution, sample the new topics, and finally update the count matrixes."
      ],
      "metadata": {
        "id": "rLr2aqgNc8MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numba as nb\n",
        "\n",
        "@nb.jit(nopython=True)\n",
        "def sample_topic(n_topics, p):\n",
        "    # NOTE: The function random.choice with argument p is unsupported in NumBa\n",
        "    # for nopython compilation. Hence, we rely on an alternative implementation\n",
        "    # which exploits the cumulative distribution of p and an uniform\n",
        "    # distribution (which is available in NumBa).\n",
        "    # return np.random.choice(n_topics, p=p)\n",
        "    return np.searchsorted(np.cumsum(p), np.random.rand(n_topics))[0]\n",
        "\n",
        "@nb.jit(forceobj=True, cache=True, parallel=False)\n",
        "def init_counts(n_topics, n_docs, documents, topics, nmz, nzw, nz, init_topics=False):\n",
        "    for m in range(n_docs):\n",
        "        # Retrieve vocab index for i-th word in document m.\n",
        "        for w in documents[m]:\n",
        "            # We reached a PAD token, assigned to -1, so we go to next document\n",
        "            if w < 0:\n",
        "                continue\n",
        "            # Get random topic assignment, i.e. z = ...\n",
        "            if init_topics:\n",
        "                z = np.random.choice(n_topics) # Uniform distribution at init\n",
        "            else:\n",
        "                z = topics[m, w] # pre-initialized at random, just read it\n",
        "            # Increment count matrices\n",
        "            nmz[m, z] += 1\n",
        "            nzw[z, w] += 1\n",
        "            nz[z] += 1\n",
        "            if init_topics:\n",
        "                # Store topic assignment, i.e. topics[(m,i)]=z\n",
        "                topics[m, w] = int(z)\n",
        "\n",
        "\n",
        "@nb.jit(nopython=True)\n",
        "def conditional(alpha, beta, nmz, nzw, nz, n_words, m, w):\n",
        "    dist = (nmz[m, :] + alpha) * (nzw[:, w] + beta) / (nz + beta * n_words)\n",
        "    return dist / np.sum(dist)\n",
        "\n",
        "@nb.jit(nopython=True)\n",
        "def sample(alpha, beta, n_words, n_docs, n_topics, documents, topics, nmz, nzw, nz):\n",
        "    for m in range(n_docs):\n",
        "        # Retrieve vocab index for i-th word in document m.\n",
        "        for w in documents[m]:\n",
        "            # We reached a PAD token, assigned to -1, so we go to next document\n",
        "            if w < 0:\n",
        "                continue\n",
        "            # Retrieve topic assignment for i-th word in document m.\n",
        "            z = topics[m, w]\n",
        "            # Decrement count matrices.\n",
        "            nmz[m, z] -= 1\n",
        "            nzw[z, w] -= 1\n",
        "            nz[z] -= 1\n",
        "            # Get conditional distribution.\n",
        "            p = conditional(alpha, beta, nmz, nzw, nz, n_words, m, w)\n",
        "            # Sample new topic assignment.\n",
        "            z = sample_topic(n_topics, p)\n",
        "            # Increment count matrices.\n",
        "            nmz[m, z] += 1\n",
        "            nzw[z, w] += 1\n",
        "            nz[z] += 1\n",
        "            # Store new topic assignment.\n",
        "            topics[m, w] = int(z)"
      ],
      "metadata": {
        "id": "fntH15Oo0AYS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once defined, the functions are compiled, cached, and tested with random data."
      ],
      "metadata": {
        "id": "OOv8oKsnhiD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1\n",
        "beta = 0.01\n",
        "n_docs = 3000\n",
        "n_topics = 10\n",
        "n_words = 2000\n",
        "doc_max_len = 1000\n",
        "\n",
        "documents = np.random.choice(n_words, size=(n_docs, doc_max_len)).astype(np.int32)\n",
        "topics = np.random.choice(n_topics, size=(n_docs, n_words)).astype(np.int32)\n",
        "nmz = np.zeros((n_docs, n_topics)).astype(np.int32)\n",
        "nzw = np.zeros((n_topics, n_words)).astype(np.int32)\n",
        "nz = np.zeros((n_topics)).astype(np.int32)\n",
        "\n",
        "init_counts(n_topics, n_docs, documents, topics, nmz, nzw, nz)\n",
        "sample(alpha, beta, n_words, n_docs, n_topics, documents, topics, nmz, nzw, nz)\n",
        "sample(alpha, beta, n_words, n_docs, n_topics, documents, topics, nmz, nzw, nz)\n",
        "sample(alpha, beta, n_words, n_docs, n_topics, documents, topics, nmz, nzw, nz)\n",
        "\n",
        "np.allclose(nmz, np.zeros((n_docs, n_topics)).astype(np.int32))"
      ],
      "metadata": {
        "id": "QeE5EwIpcBmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b01c73-5847-4b9c-bbec-7675fe3675ee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-58c58fc7d4f7>:12: NumbaWarning: Cannot cache compiled function \"init_counts\" as it uses lifted code\n",
            "  @nb.jit(forceobj=True, cache=True, parallel=False)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA Class"
      ],
      "metadata": {
        "id": "cyVrCuBtGwLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LDACGS:\n",
        "    \"\"\"Do LDA with Collapsed Gibbs Sampling.\"\"\"\n",
        "    def __init__(self, n_topics=2, alpha=0.1, beta=0.01, corpus=None):\n",
        "        \"\"\"Initialize system parameters.\"\"\"\n",
        "        self.n_topics = n_topics\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.vocab_built = False\n",
        "        if corpus:\n",
        "            self.buildVocabulary(corpus)\n",
        "            self.vocab_built = True\n",
        "\n",
        "    def buildVocabulary(self, corpus):\n",
        "        \"\"\"Given a corpus as a list of list of words, build the vocabulary.\"\"\"\n",
        "        self.corpus = corpus\n",
        "        n_docs = len(corpus)\n",
        "        self.vocab = list({v for doc in corpus for v in doc})\n",
        "        # ======================================================================\n",
        "        # Transform the words in the corpus to indexes to the words in the\n",
        "        # vocabulary\n",
        "        # ======================================================================\n",
        "        self.documents = []\n",
        "        self.doc_max_len = 0\n",
        "        for i in range(n_docs):\n",
        "            self.documents.append({})\n",
        "            for j in range(len(corpus[i])):\n",
        "                if corpus[i][j] in self.vocab:\n",
        "                    self.documents[i][j] = self.vocab.index(corpus[i][j])\n",
        "            if len(self.documents[i]) > self.doc_max_len:\n",
        "                self.doc_max_len = len(self.documents[i])\n",
        "        print(f'INFO. Number of documents in corpus: {len(self.documents)}')\n",
        "        print(f'INFO. Vocabulary size: {len(self.vocab)}')\n",
        "        self.vocab_built = True\n",
        "\n",
        "    def initialize(self, n_topics=None):\n",
        "        \"\"\"Initialize the three count matrices.\"\"\"\n",
        "        if n_topics:\n",
        "            self.n_topics = n_topics    \n",
        "        self.n_words = len(self.vocab)\n",
        "        self.n_docs = len(self.documents)\n",
        "        # ======================================================================\n",
        "        # Convert list of dictionaries to pure numpy arrays\n",
        "        # ======================================================================\n",
        "        # NOTE: Init document matrix with PAD tokens, i.e. -1\n",
        "        self.documents_np = -np.ones((self.n_docs, self.doc_max_len),\n",
        "                                     dtype=np.int32)\n",
        "        for i in range(self.n_docs):\n",
        "            vals = list(self.documents[i].values())\n",
        "            for j in range(len(vals)):\n",
        "                self.documents_np[i][j] = int(vals[j])\n",
        "        # ======================================================================\n",
        "        # Instantiate the three count matrices\n",
        "        # ======================================================================\n",
        "        # The (i, j) entry of self.nmz is the number of words in document i\n",
        "        # assigned to topic j.\n",
        "        self.nmz = np.zeros((self.n_docs, self.n_topics), dtype=np.int32)\n",
        "        # The (i, j) entry of self.nzw is the number of times word j is\n",
        "        # assigned to topic i.\n",
        "        self.nzw = np.zeros((self.n_topics, self.n_words), dtype=np.int32)\n",
        "        # The (i)-th entry is the number of times topic i is assigned in the\n",
        "        # corpus.\n",
        "        self.nz = np.zeros(self.n_topics, dtype=np.int32)\n",
        "        # Initialize the topic assignment matrix to uniform random topics\n",
        "        sz = (self.n_docs, self.n_words)\n",
        "        self.topics = np.random.choice(self.n_topics, size=sz).astype(np.int32)\n",
        "        # ======================================================================\n",
        "        # Initialize the topics and the three count matrices\n",
        "        # ======================================================================\n",
        "        init_counts(self.n_topics, self.n_docs, self.documents_np, self.topics,\n",
        "                    self.nmz, self.nzw, self.nz)\n",
        "\n",
        "    def sample(self, corpus=None, burnin=100, n_topics=None, alpha=None,\n",
        "               beta=None, verbose=0):\n",
        "        \"\"\"Run collapsed Gibbs sampling iterations\"\"\"\n",
        "        if alpha:\n",
        "            self.alpha = alpha\n",
        "        if beta:\n",
        "            self.beta = beta\n",
        "        if not self.vocab_built:\n",
        "            if verbose:\n",
        "                print(f'INFO. Building corpus.')\n",
        "            self.buildVocabulary(corpus)\n",
        "        if verbose:\n",
        "            print(f'INFO. Vocabulary built. Initializing.')\n",
        "        # ======================================================================\n",
        "        # Initialize count matrices\n",
        "        # ======================================================================\n",
        "        self.initialize(n_topics)\n",
        "        # ======================================================================\n",
        "        # Perform sampling\n",
        "        # ======================================================================\n",
        "        if verbose:\n",
        "            print(f'INFO. Starting Burn-in sampling.')\n",
        "        for i in tqdm(range(burnin)):\n",
        "            sample(self.alpha, self.beta, self.n_words, self.n_docs,\n",
        "                  self.n_topics, self.documents_np, self.topics, self.nmz,\n",
        "                  self.nzw, self.nz)\n",
        "        # ======================================================================\n",
        "        # After discarding the initial samples, make one actual sampling\n",
        "        # ======================================================================\n",
        "        sample(self.alpha, self.beta, self.n_words, self.n_docs,\n",
        "                self.n_topics, self.documents_np, self.topics, self.nmz,\n",
        "                self.nzw, self.nz)\n",
        "    \n",
        "    def D(self, widx, cowidx=None):\n",
        "        \"\"\"\n",
        "        Get the number of documents in which the word index appears at least\n",
        "        once\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        widx : int\n",
        "            Vocabulary index of the word to count\n",
        "        cowidx : int, optional\n",
        "            Vocabulary index of the co-word to count\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            the number of documents in which the word index appears at least\n",
        "            once, if coidx is not supplied, else the number of documents in\n",
        "            which they both appear together\n",
        "        \"\"\"\n",
        "        matches = np.bitwise_or.reduce(self.documents_np == widx, axis=1)\n",
        "        if cowidx:\n",
        "            comatches = np.bitwise_or.reduce(self.documents_np == cowidx, axis=1)\n",
        "            return np.logical_and(matches, comatches).sum()\n",
        "        else:\n",
        "            return matches.sum()\n",
        "    \n",
        "    def Umass(self, topics):\n",
        "        C = np.zeros(len(topics))\n",
        "        for i, topic_words in enumerate(topics):\n",
        "            for vm, vl in itertools.combinations(topic_words, r=2):\n",
        "                C[i] += np.log((self.D(vm, vl) + 1) / self.D(vm))\n",
        "        return C\n",
        "    \n",
        "    def getTopTerms(self, n_terms=10):\n",
        "        \"\"\"Get the list of the top-n_terms words per topic\"\"\"\n",
        "        # ======================================================================\n",
        "        # Get topics distribution Phi, with shape: (n_topics, n_words)\n",
        "        # ======================================================================\n",
        "        phi = self.nzw + self.beta\n",
        "        phi = phi / (np.sum(phi, axis=1)[:, np.newaxis])\n",
        "        # ======================================================================\n",
        "        # Sort probabilities per topic, i.e. row, and get the ordered word\n",
        "        # indexes in the vocabulary\n",
        "        # ======================================================================\n",
        "        idx = np.argsort(phi, axis=1).astype(np.int32)\n",
        "        # ======================================================================\n",
        "        # Finally, select the top-N words for each fo the topics\n",
        "        # ======================================================================\n",
        "        topics, topics_idx = [], []\n",
        "        for k in range(self.n_topics):\n",
        "            topics.append([self.vocab[idx[k, self.n_words - 1 - i]] for i in range(n_terms)])\n",
        "            topics_idx.append([idx[k, n_words - 1 - i] for i in range(n_terms)])\n",
        "        return topics, self.Umass(topics_idx)"
      ],
      "metadata": {
        "id": "puBDXUpap61E"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ldacgs = LDACGS(corpus=cleaned_corpus)"
      ],
      "metadata": {
        "id": "K6dMmilYtoC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52449e04-c20b-4803-c3d5-0294e3bdb7c6"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO. Number of documents in corpus: 4000\n",
            "INFO. Vocabulary size: 3643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beta = 0.1 # 0.1 or 0.01\n",
        "n_topics = 10 # 10 or 50\n",
        "burnin = 100 # 100 or 200\n",
        "ldacgs.sample(burnin=burnin, beta=beta, n_topics=n_topics, verbose=0)"
      ],
      "metadata": {
        "id": "qckjcJuyF1rS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f11b058-1d5d-42c9-9c18-8d6e66519715"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:50<00:00,  3.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topics, u_mass = ldacgs.getTopTerms(n_terms=10)\n",
        "for i, topic in enumerate(topics):\n",
        "    print(f'Topic n.{i}: {\", \".join(t for t in topic)}')\n",
        "print(f'Umass coherence score: {u_mass.mean()}')"
      ],
      "metadata": {
        "id": "z4MUFnUet7H1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c913a1ff-64de-4309-cc23-e07697a617c3"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic n.0: said, year, would, two, today, profit, expected, february, major, tonnes\n",
            "Topic n.1: net, group, shares, told, agreement, international, total, offer, four, japan\n",
            "Topic n.2: may, nine, april, price, march, prices, end, operations, business, higher\n",
            "Topic n.3: also, loss, government, exchange, increase, month, current, rate, securities, agreed\n",
            "Topic n.4: share, first, five, oil, six, record, due, earnings, since, sale\n",
            "Topic n.5: mln, shr, company, revs, shrs, ltd, interest, tax, unit, says\n",
            "Topic n.6: cts, market, bank, week, made, ended, assets, cash, shareholders, chairman\n",
            "Topic n.7: inc, qtr, corp, note, last, mths, co, per, avg, june\n",
            "Topic n.8: dlrs, billion, one, stock, trade, could, months, years, buy, rise\n",
            "Topic n.9: vs, pct, sales, new, three, dlr, added, foreign, president, statement\n",
            "Umass coherence score: -186.54882996969266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $U_{MASS}$ Coherence Score"
      ],
      "metadata": {
        "id": "Q4a6gVQ3wolV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The $U_{MASS}$ coherence score is defined as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "    C(t; V^{(t)})= \\sum_{m=2}^M \\sum_{l=1}^{m-1} \\log \\frac{\\mathbf{D}\\big(v_m^{(t)}, v_l^{(t)}\\big) + 1}{\\mathbf{D}\\big(v_l^{(t)}\\big)}\n",
        "\\end{equation}\n",
        "\n",
        "where $\\mathbf{D}\\big(v_m^{(t)},\\, v_l^{(t)}\\big)$ represents the number of documents $d$ which contain at least one each of the words $v_m$ and $v_l$ for topic $t$.\n",
        "\n",
        "The following implementations have been later embedded in the `LDACGS` class."
      ],
      "metadata": {
        "id": "b2NqHw5VYoai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "beta = ldacgs.beta\n",
        "n_topics = ldacgs.n_topics\n",
        "vocab = ldacgs.vocab\n",
        "documents = ldacgs.documents_np\n",
        "nzw = ldacgs.nzw\n",
        "\n",
        "def D(widx, documents, cowidx=None):\n",
        "    \"\"\"Get the number of documents in which the word index appears at least once\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    widx : int\n",
        "        Vocabulary index of the word to count\n",
        "    documents : numpy array of int32\n",
        "        Corpus containing words as vocabulary indexes, shape: (n_docs, max_doc_len)\n",
        "    cowidx : int, optional\n",
        "        Vocabulary index of the co-word to count\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        the number of documents in which the word index appears at least once,\n",
        "        if coidx is not supplied, else the number of documents in which they\n",
        "        both appear together\n",
        "    \"\"\"\n",
        "    matches = np.bitwise_or.reduce(documents == widx, axis=1)\n",
        "    if cowidx:\n",
        "        comatches = np.bitwise_or.reduce(documents == cowidx, axis=1)\n",
        "        return np.logical_and(matches, comatches).sum()\n",
        "    else:\n",
        "        return matches.sum()\n",
        "\n",
        "\n",
        "def get_top_topics(nzw, vocab, n_terms=10, beta=0.1):\n",
        "    n_topics = nzw.shape[0]\n",
        "    n_words = len(vocab)\n",
        "    # Get topics distribution Phi, with shape: (n_topics, n_words)\n",
        "    phi = nzw + beta\n",
        "    phi = phi / (np.sum(phi, axis=1)[:, np.newaxis])\n",
        "    # Sort probabilities and get the ordered word indexes in the vocabulary\n",
        "    widx = np.argsort(phi, axis=1).astype(np.int32)\n",
        "    # Finally, select the top-N words for each fo the topics\n",
        "    topics, topics_idx = [], []\n",
        "    for k in range(n_topics):\n",
        "        topics.append([vocab[widx[k, n_words - 1 - i]] for i in range(n_terms)])\n",
        "        topics_idx.append([widx[k, n_words - 1 - i] for i in range(n_terms)])\n",
        "    return topics, topics_idx\n",
        "\n",
        "topics, topics_idx = get_top_topics(nzw, vocab, 10, beta)\n",
        "for i, topic in enumerate(topics):\n",
        "    print(f'Topic n.{i}: {\", \".join(t for t in topic)}')\n",
        "\n",
        "\n",
        "@nb.jit(nopython=True)\n",
        "def combinations(pool, r):\n",
        "    n = len(pool)\n",
        "    indices = list(range(r))\n",
        "    empty = not(n and (0 < r <= n))\n",
        "    if not empty:\n",
        "        result = [pool[i] for i in indices]\n",
        "        yield result\n",
        "    while not empty:\n",
        "        i = r - 1\n",
        "        while i >= 0 and indices[i] == i + n - r:\n",
        "            i -= 1\n",
        "        if i < 0:\n",
        "            empty = True\n",
        "        else:\n",
        "            indices[i] += 1\n",
        "            for j in range(i+1, r):\n",
        "                indices[j] = indices[j-1] + 1\n",
        "            result = [pool[i] for i in indices]\n",
        "            yield result\n",
        "\n",
        "\n",
        "def Umass(topics, documents):\n",
        "    C = np.zeros(len(topics))\n",
        "    for i, topic_words in enumerate(topics):\n",
        "        for vm, vl in itertools.combinations(topic_words, r=2):\n",
        "            C[i] += np.log((D(vm, documents, vl) + 1) / D(vm, documents))\n",
        "    return C\n",
        "\n",
        "u_mass = Umass(np.array(topics_idx), documents)\n",
        "print(f'Umass coherence score: {u_mass.mean()}')"
      ],
      "metadata": {
        "id": "fEVb8sEiIiwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "kmyzy2y32R3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_data = []\n",
        "for corpus_name in corpora.keys():\n",
        "    # Get previously tokenized and cleaned corpus\n",
        "    cleaned_corpus = corpora[corpus_name]\n",
        "    # Instantiate LDACGS, i.e. build the corpus vocabulary\n",
        "    ldacgs = LDACGS(corpus=cleaned_corpus)\n",
        "    # Evaluate all combinations\n",
        "    for K in [10, 50]:\n",
        "        for burnin in [100, 200]:\n",
        "            for alpha, beta in [(0.1, 0.1), (0.1, 0.01)]:\n",
        "                # Perform Gibbs sampling\n",
        "                ldacgs.sample(burnin=burnin, n_topics=K, alpha=alpha, beta=beta)\n",
        "                # Get topics\n",
        "                topics, u_mass = ldacgs.getTopTerms(n_terms=20)\n",
        "                # Dump topics and Umass per topic to file\n",
        "                topicfile = f'topics_{corpus_name}_{K}_{burnin}_{alpha}_{beta}.txt'\n",
        "                with open(topicfile, 'w') as f:\n",
        "                    for i, topic in enumerate(topics):\n",
        "                        f.write(f'Topic n.{i} (Umass = {u_mass[i]:.3f}): {\", \".join(t for t in topic)}\\n')\n",
        "                # Populate table entries\n",
        "                print(f'Umass coherence score for {K} topics and burn-in {burnin} (α={alpha}, β={beta}): {u_mass.mean():.3f}')\n",
        "                table_data.append((corpus_name, K, burnin, alpha, beta, round(u_mass.mean(), 3)))\n",
        "cols = ['Corpus', 'N. Topics', 'Burn-in', 'Alpha', 'Beta', 'Umass Score']\n",
        "table = pd.DataFrame(table_data, columns=cols)\n",
        "table.to_csv(f'collapsed_gibbs_results.csv', encoding='utf-8', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sCHXAEJk5L6",
        "outputId": "6e81bcce-8b72-48eb-bd6b-6d80b314a62b"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO. Number of documents in corpus: 4000\n",
            "INFO. Vocabulary size: 3643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:25<00:00,  3.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 10 topics and burn-in 100 (α=0.1, β=0.1): -504.108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 10 topics and burn-in 100 (α=0.1, β=0.01): -516.453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:51<00:00,  3.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 10 topics and burn-in 200 (α=0.1, β=0.1): -502.423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:52<00:00,  3.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 10 topics and burn-in 200 (α=0.1, β=0.01): -507.965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:12<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 50 topics and burn-in 100 (α=0.1, β=0.1): -558.461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:13<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 50 topics and burn-in 100 (α=0.1, β=0.01): -627.397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:23<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 50 topics and burn-in 200 (α=0.1, β=0.1): -534.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:25<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 50 topics and burn-in 200 (α=0.1, β=0.01): -616.861\n",
            "INFO. Number of documents in corpus: 7532\n",
            "INFO. Vocabulary size: 8577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:55<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 10 topics and burn-in 100 (α=0.1, β=0.1): -503.679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 10 topics and burn-in 100 (α=0.1, β=0.01): -516.199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:52<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 10 topics and burn-in 200 (α=0.1, β=0.1): -500.985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:51<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 10 topics and burn-in 200 (α=0.1, β=0.01): -538.994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [02:40<00:00,  1.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 50 topics and burn-in 100 (α=0.1, β=0.1): -487.593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [02:41<00:00,  1.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 50 topics and burn-in 100 (α=0.1, β=0.01): -488.088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [05:21<00:00,  1.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 50 topics and burn-in 200 (α=0.1, β=0.1): -482.744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [05:17<00:00,  1.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umass coherence score for 50 topics and burn-in 200 (α=0.1, β=0.01): -483.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table"
      ],
      "metadata": {
        "id": "WQr_GPux4Bdy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "00e26857-3797-4c3f-ec02-42976614a7d8"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Corpus  N. Topics  Burn-in  Alpha  Beta  Umass Score\n",
              "0        reuters         10      100    0.1  0.10     -504.108\n",
              "1        reuters         10      100    0.1  0.01     -516.453\n",
              "2        reuters         10      200    0.1  0.10     -502.423\n",
              "3        reuters         10      200    0.1  0.01     -507.965\n",
              "4        reuters         50      100    0.1  0.10     -558.461\n",
              "5        reuters         50      100    0.1  0.01     -627.397\n",
              "6        reuters         50      200    0.1  0.10     -534.134\n",
              "7        reuters         50      200    0.1  0.01     -616.861\n",
              "8   20newsgroups         10      100    0.1  0.10     -503.679\n",
              "9   20newsgroups         10      100    0.1  0.01     -516.199\n",
              "10  20newsgroups         10      200    0.1  0.10     -500.985\n",
              "11  20newsgroups         10      200    0.1  0.01     -538.994\n",
              "12  20newsgroups         50      100    0.1  0.10     -487.593\n",
              "13  20newsgroups         50      100    0.1  0.01     -488.088\n",
              "14  20newsgroups         50      200    0.1  0.10     -482.744\n",
              "15  20newsgroups         50      200    0.1  0.01     -483.105"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2891c1c8-2c2c-448f-a730-ab944937c7db\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Corpus</th>\n",
              "      <th>N. Topics</th>\n",
              "      <th>Burn-in</th>\n",
              "      <th>Alpha</th>\n",
              "      <th>Beta</th>\n",
              "      <th>Umass Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>reuters</td>\n",
              "      <td>10</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-504.108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>reuters</td>\n",
              "      <td>10</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-516.453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>reuters</td>\n",
              "      <td>10</td>\n",
              "      <td>200</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-502.423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>reuters</td>\n",
              "      <td>10</td>\n",
              "      <td>200</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-507.965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>reuters</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-558.461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>reuters</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-627.397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>reuters</td>\n",
              "      <td>50</td>\n",
              "      <td>200</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-534.134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>reuters</td>\n",
              "      <td>50</td>\n",
              "      <td>200</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-616.861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>20newsgroups</td>\n",
              "      <td>10</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-503.679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>20newsgroups</td>\n",
              "      <td>10</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-516.199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>20newsgroups</td>\n",
              "      <td>10</td>\n",
              "      <td>200</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-500.985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>20newsgroups</td>\n",
              "      <td>10</td>\n",
              "      <td>200</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-538.994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>20newsgroups</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-487.593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>20newsgroups</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-488.088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>20newsgroups</td>\n",
              "      <td>50</td>\n",
              "      <td>200</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-482.744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>20newsgroups</td>\n",
              "      <td>50</td>\n",
              "      <td>200</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-483.105</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2891c1c8-2c2c-448f-a730-ab944937c7db')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2891c1c8-2c2c-448f-a730-ab944937c7db button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2891c1c8-2c2c-448f-a730-ab944937c7db');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip collapsed_gibbs_results.zip collapsed_gibbs_results.csv topics_*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TM22lAgnh4F",
        "outputId": "26de5b28-a48f-4c2c-ca01-134e7d1234d7"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: collapsed_gibbs_results.csv (deflated 66%)\n",
            "  adding: topics_20newsgroups_10_100_0.1_0.01.txt (deflated 55%)\n",
            "  adding: topics_20newsgroups_10_100_0.1_0.1.txt (deflated 55%)\n",
            "  adding: topics_20newsgroups_10_200_0.1_0.01.txt (deflated 55%)\n",
            "  adding: topics_20newsgroups_10_200_0.1_0.1.txt (deflated 55%)\n",
            "  adding: topics_20newsgroups_50_100_0.1_0.01.txt (deflated 60%)\n",
            "  adding: topics_20newsgroups_50_100_0.1_0.1.txt (deflated 60%)\n",
            "  adding: topics_20newsgroups_50_200_0.1_0.01.txt (deflated 60%)\n",
            "  adding: topics_20newsgroups_50_200_0.1_0.1.txt (deflated 60%)\n",
            "  adding: topics_reuters_10_100_0.1_0.01.txt (deflated 55%)\n",
            "  adding: topics_reuters_10_100_0.1_0.1.txt (deflated 54%)\n",
            "  adding: topics_reuters_10_200_0.1_0.01.txt (deflated 54%)\n",
            "  adding: topics_reuters_10_200_0.1_0.1.txt (deflated 54%)\n",
            "  adding: topics_reuters_50_100_0.1_0.01.txt (deflated 61%)\n",
            "  adding: topics_reuters_50_100_0.1_0.1.txt (deflated 61%)\n",
            "  adding: topics_reuters_50_200_0.1_0.01.txt (deflated 61%)\n",
            "  adding: topics_reuters_50_200_0.1_0.1.txt (deflated 61%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ittbTZ1cza7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4031243f-1b00-40be-abbd-a592549f6342"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "drive_dir = os.path.join(os.path.abspath(''), 'drive', 'MyDrive')\n",
        "!cp collapsed_gibbs_results.zip $drive_dir"
      ],
      "metadata": {
        "id": "uLWxcDngtvd2"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gibbs Sampling: Alternative 1"
      ],
      "metadata": {
        "id": "VVGc4hLOG2Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PARAMETERS: alpha, beta, K, D, p\n",
        "# (where p is the Cat(p1, p2, ..., p_K) distribution initialized uniformly)\n",
        "alpha = 0.1\n",
        "beta = 0.1\n",
        "K = 50\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = list({v for doc in cleaned_corpus for v in doc})\n",
        "vocab.sort()\n",
        "# Storing the topic assigned to each word in the corpus\n",
        "# format:\n",
        "#   row 1: document ID (index)\n",
        "#   row 2: word ID (w/in the vocab)\n",
        "#   row 3: word topic k\n",
        "topic_mat = []\n",
        "for d, doc in enumerate(cleaned_corpus):\n",
        "    for word in doc:\n",
        "        topic_mat.append((d, vocab.index(word), int(np.random.choice(a=K))))\n",
        "topic_mat = np.array(topic_mat, dtype=np.int32).T\n",
        "\n",
        "# Define Numpy count matrices for n and m\n",
        "D = len(cleaned_corpus) + 1 # Number of documents\n",
        "V = len(vocab)\n",
        "n_d = np.zeros((D, K), dtype=np.int32)\n",
        "m_k = np.zeros((V, K), dtype=np.int32)\n",
        "\n",
        "topic_mat.shape\n",
        "\n",
        "# W = len(df) # Total number of words in the corpus\n",
        "# V = len(df.word_id.unique()) # Number of words in the vocabulary\n",
        "# D = df.document.max() + 1 # Number of documents\n",
        "# # Initialize topics k randomly from p\n",
        "# df['z'] = np.random.choice(a=K, size=W, p=p)\n",
        "# # Storing the topic assigned to each word in the corpus\n",
        "# # format:\n",
        "# #   row 1: document ID (index)\n",
        "# #   row 2: word ID (w/in the vocab)\n",
        "# #   row 3: word topic k\n",
        "# topic_mat = df[['document', 'word_id', 'z']].to_numpy(dtype=np.int32).T"
      ],
      "metadata": {
        "id": "6t5MqBrEG5hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import jit, float32\n",
        "\n",
        "\"\"\"\n",
        "Calculating n_d and m_k:\n",
        "Given topic_mat : \n",
        "    - row 0 == indicators for document \n",
        "    - row 1 == indicators for word (word_id)\n",
        "    - row 2 == z, i.e. k-value for each word\n",
        "\n",
        "n_d implementation :\n",
        "    vectorize search for document + topic matches in document\n",
        "    (i.e unique row 0 + row 2 matches)\n",
        "    let position n_d[doc, topic] == # positions in doc d with topic k\n",
        "\n",
        "m_k implementation :\n",
        "    vectorize search for word + topic matches in corpus \n",
        "    (i.e unique row 1 + row 2 matches)\n",
        "    let position m_k[word, topic] == # positions in corpus with\n",
        "    topic k and word w\n",
        "\"\"\"\n",
        "@jit(forceobj=True, cache=True)\n",
        "def init_count_mat(topic_mat, n_d, m_k):\n",
        "    # Populate n and m \n",
        "    # n_d\n",
        "    vals, counts = np.unique(topic_mat[[0, 2], :], return_counts=True, axis=1)\n",
        "    n_d[vals[0], vals[1]] = counts\n",
        "    # m_k\n",
        "    val, count = np.unique(topic_mat[[1, 2], :], return_counts=True, axis=1)\n",
        "    m_k[val[0], val[1]] = count\n",
        "\n",
        "# @jit(nopython=True, cache=True)\n",
        "def draw_from_prob_matrix(prob_matrix, axis=1):\n",
        "    \"\"\"\n",
        "    With this implementation we get probability matrix p with shape (V, K).\n",
        "    The function lets us sample by drawing one value from each distribution\n",
        "    according to the distribution at row w_dj.\n",
        "    \"\"\"\n",
        "    r = np.expand_dims(np.random.rand(prob_matrix.shape[1-axis]), axis=axis)\n",
        "    return (prob_matrix.cumsum(axis=axis) > r).argmax(axis=axis)"
      ],
      "metadata": {
        "id": "c3gY_VF5IEWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @jit(nopython=True, cache=True, parallel=False)\n",
        "def sample_alt1(n_iter, topic_mat, n_d, m_k):\n",
        "    DOCS = 0\n",
        "    WORDS = 1\n",
        "    TOPICS = 2\n",
        "    # Gibbs sampling with n_iter iterations\n",
        "    for i in tqdm(range(n_iter)):\n",
        "        # Update probabilities and draw samples for k\n",
        "        p = n_d[topic_mat[DOCS]] * m_k[topic_mat[WORDS], :]\n",
        "        p = p / p.sum(axis=WORDS, keepdims=True)\n",
        "        topic_mat[TOPICS] = draw_from_prob_matrix(p)\n",
        "        # Update n_d\n",
        "        n_vals, n_counts = np.unique(topic_mat[[DOCS, TOPICS], :], return_counts=True, axis=WORDS)\n",
        "        n_d[n_vals[DOCS], n_vals[WORDS]] = n_counts\n",
        "        # Update m_k\n",
        "        m_vals, m_counts = np.unique(topic_mat[WORDS:, :], return_counts=True, axis=WORDS)\n",
        "        m_k[m_vals[DOCS], m_vals[WORDS]] = m_counts\n",
        "    p = n_d[topic_mat[DOCS]] * m_k[topic_mat[WORDS], :]\n",
        "    return p / p.sum(axis=WORDS, keepdims=True)"
      ],
      "metadata": {
        "id": "0M6DuV7dIHux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iter = 200\n",
        "init_count_mat(topic_mat, n_d, m_k)\n",
        "z = sample_alt1(n_iter, topic_mat, n_d, m_k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k-iK9sxNmCd",
        "outputId": "4f797616-fa27-45e7-99ed-cfa2e94c6e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:49<00:00,  1.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(topic_mat.shape)\n",
        "print(z.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8Qh9T_LNpL8",
        "outputId": "0ca04a49-4fdb-4de2-c482-3f325de0e05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 274494)\n",
            "(274494, 50)\n"
          ]
        }
      ]
    }
  ]
}