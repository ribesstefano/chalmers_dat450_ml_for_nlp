{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ribesstefano/chalmers_dat450_ml_for_nlp/blob/main/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXVM2mFqUl5C"
      },
      "source": [
        "# Assignment 1: Political stance classification\n",
        "\n",
        "In this assignment, we will implement a system to carry out a document classification task. The documents have been harvested by students from various social media platforms (mainly comments from YouTube). Each document has been annotated (also by students) for whether it is positive or negative towards [Brexit](https://en.wikipedia.org/wiki/Brexit) (the UK leaving the European Union).\n",
        "\n",
        "The pedagical point of this assignment is not so much about the task of document classification, or about the design of neural network solutions for this task. (This will be discussed in more detail in lecture sessions.) Instead, most of your work here is intended to make sure you understand the *practical* side of working with PyTorch for NLP tasks.\n",
        "\n",
        "Most of your work will be the implementation of\n",
        "- *preprocessing* utlities to convert the text into a numerical format that can be used by PyTorch,\n",
        "- the *training loop* that takes an untrained model, applies it to a training set, and updates the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9mxgr0hd6ZS"
      },
      "source": [
        "# Preliminaries\n",
        "\n",
        "To run the code, the following libraries need to be installed on your machine:\n",
        "\n",
        "- [PyTorch](https://pytorch.org/) is the machine learning library we will use. You can see on the PyTorch home page how to install the library.\n",
        "- [scikit-learn](https://scikit-learn.org/) for a couple of minor utility functions.\n",
        "- [spaCy](https://spacy.io/) for basic linguistic preprocessing.\n",
        "- [pandas](https://pandas.pydata.org/) to read the files.\n",
        "- [tqdm](https://tqdm.github.io/) for a progress bar used in the training loop.\n",
        "- [NumPy](https://numpy.org/) to combine some matrices in the final part of the assignment.\n",
        "\n",
        "If you use Colab, nothing needs to be installed since all libraries are included in the standard setup.\n",
        "\n",
        "PyTorch is mandatory for this assignment, but the other libraries are simply for convenience and you can solve the assignment without them (if for some reason you don't want to install the libraries)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7wpMbX6Ul5X"
      },
      "source": [
        "Download the training and test files from [this directory](http://www.cse.chalmers.se/~richajo/dat450/assignments/data/). Place them in some directory where this notebook can access them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aea8GoZNUl5Z"
      },
      "outputs": [],
      "source": [
        "# The following shell commands will download the training and test files to your Colab runtime.\n",
        "!wget http://www.cse.chalmers.se/~richajo/dat450/assignments/data/brexit_train.tsv\n",
        "!wget http://www.cse.chalmers.se/~richajo/dat450/assignments/data/brexit_test.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9Lhhz7ZUl5h"
      },
      "source": [
        "### Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip8n3V-nUl5i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8Fm1AXuUl5j"
      },
      "source": [
        "We use Pandas to read the file containing the training data. It consists of two tab-separated columns, where the first column contains the gold-standard labels and the second contains the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC44--mWUl5j"
      },
      "outputs": [],
      "source": [
        "train_corpus = pd.read_csv('brexit_train.tsv', sep='\\t', header=0, names=['label', 'text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRtxL0WXUl5k"
      },
      "source": [
        "Here, we can see the first few instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAWphWWpUl5m"
      },
      "outputs": [],
      "source": [
        "train_corpus.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWKo8UalUl5n"
      },
      "source": [
        "We split the data into a training (80%) and a validation part (20%). We use the convenience function `train_test_split` from scikit-learn.\n",
        "\n",
        "Following standard notation, we refer to the input part of the data (that is, the documents) as `X` and the output part (classification labels) as `Y`.\n",
        "\n",
        "The validation will be used to compute diagnostic scores during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upAnPIKJUl5q"
      },
      "outputs": [],
      "source": [
        "Xtrain, Xval, Ytrain, Yval = train_test_split(train_corpus.text, train_corpus.label, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLdrvQ3sUl5r"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "The task of splitting a text into a sequence of symbols (*tokens*) is called *tokenization*. Classically, the tokens correspond to words and punctuation symbols. However, later in the course, we will see alternatives to word-based tokenization.\n",
        "\n",
        "We will not build our own tokenizer, but instead use the tokenizer for English built into the `spacy` library.\n",
        "\n",
        "**Please note**: the first time you use spaCy with some language (English in our case), you need to install a module for that language. See [here](https://spacy.io/usage/models) for a description of how to do this. In short, you typically need to run a command in a shell such as\n",
        "\n",
        "```\n",
        "python -m spacy download en_core_web_sm\n",
        "```\n",
        "Colab users don't need to carry out this step, since spaCy and the English module are already installed by default.\n",
        "\n",
        "When the English module is downloaded, we can load it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOpqHBhiUl5t"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZyqkAKPUl5w"
      },
      "source": [
        "Now, we have what we need to do tokenization of English texts. \n",
        "\n",
        "For your convenience, the function below calls the spaCy tokenizer and extracts the token strings. Optionally, we also apply lowercase normalization to the strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaCvuj9sUl5w"
      },
      "outputs": [],
      "source": [
        "def tokenize(text, lowercase=True):\n",
        "    if lowercase:\n",
        "        return [t.text.lower() for t in nlp.tokenizer(text)]\n",
        "    else:\n",
        "        return [t.text for t in nlp.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITR4bbWQUl5y"
      },
      "source": [
        "Let's apply the tokenization function to an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhgDcezYUl5z"
      },
      "outputs": [],
      "source": [
        "tokenize('[12345/689-123] L. Ron Hubbard went to the U.S... \"He joined the U.S. Army!!!\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47EKP9WMUl50"
      },
      "source": [
        "### Example: how to find the most frequent words in a dataset\n",
        "\n",
        "When you implement the vocabulary processing below, you will need to compute word frequencies. This can of course be done using standard Python data structures, but the easiest approach is probably to use the specialized dictionary type called [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter). As the name suggests, this is used in Python when counting things.\n",
        "\n",
        "Here are a few idioms showing how to use the `Counter`. The examples show three different ways to compute the frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWBLICRRb3Nf"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "freqs = Counter()\n",
        "for x in Xtrain:\n",
        "    for t in tokenize(x):\n",
        "        freqs[t] += 1\n",
        "\n",
        "#freqs = Counter()\n",
        "#for x in Xtrain:\n",
        "#    freqs.update(tokenize(x))\n",
        "\n",
        "#freqs = Counter(t for x in Xtrain for t in tokenize(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX5LZVc5Ul52"
      },
      "source": [
        "After building the `Counter`, we have a data structure where each word is mapped to a frequency count.\n",
        "\n",
        "We can then use the method `most_common` to find the items in the dictionary that have the highest frequencies. This method returns a sorted list of item/frequency pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA-Kla_bUl52"
      },
      "outputs": [],
      "source": [
        "for word, freq in freqs.most_common(10):\n",
        "    print(word, freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dHCj7jefUvz"
      },
      "source": [
        "# Part 1: Preprocessing documents\n",
        "\n",
        "Now, we are ready to implement the utilities we need in order to preprocess documents for machine learning with PyTorch.\n",
        "\n",
        "Your implementation will be done in this class `DocumentPreprocessor`. *Please note* that there are some tests below that check whether you seem to have implemented the methods correctly. If you want, you can work incrementally, so that you make sure that your tests run before moving on to the next step.\n",
        "\n",
        "**Your work:**\n",
        "\n",
        "**1)** Implement the method `build_vocab`. \n",
        "\n",
        "This method takes a training set (inputs `X` and outputs `Y`) and builds two vocabularies, one for the words in the input documents and one for the output labels. These vocabularies are data structures that allow you to map a string to a corresponding integer index.\n",
        "\n",
        "**Requirements:**\n",
        "- The special symbols `PAD` and `UNKNOWN` should correspond to the encoded values 0 and 1, respectively.\n",
        "- The size of the resulting vocabulary should be at most `max_voc_size`, if the user has provided a value of this parameter. If you observe more unique words than `max_voc_size`-2, then you should only include the most frequent words.\n",
        "\n",
        "You can use any data structures you want in this step, but probably you will use some sort of dictionaries. For the `Y` vocabulary, the scikit-learn utility [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) can optionally be used, but a regular dictionary is also OK.\n",
        "\n",
        "**2)** Implement `n_classes` and `voc_size`. This should be trivial after you have solved the previous task.\n",
        "\n",
        "***Testing.*** You can now run the first tests below to validate your implementation, before you proceed to the next task.\n",
        "\n",
        "**3)** Implement `encode`.\n",
        "\n",
        "This method takes a list of input documents `X` and output labels `Y` and returns a list of encoded `x`-`y` pairs, where the `x` part is a list of integers and the `y` part is an integer, using the string-to-integer mappings you created in `build_vocab`. For instance, we could hypothetically have something like\n",
        "\n",
        "```\n",
        "X = ['Leave now!'], Y = ['pro']  ==>  [([75, 34, 14], 1)]\n",
        "```\n",
        "\n",
        "**Requirements:**\n",
        "- If the user provided a value of the hyperparameter `max_len`, then any document that is longer than this value needs to be truncated.\n",
        "- For words that are not included in the vocabulary, the special symbol `UNKNOWN` (hard-coded to index 1) should be used.\n",
        "\n",
        "**4)** Implement `decode_predictions`.\n",
        "\n",
        "This method simply inverts the symbol-to-integer mapping we use to encode the `Y` values. So we could have something like\n",
        "\n",
        "```\n",
        "[0, 1, 1]  ==>  ['anti', 'pro', 'pro']\n",
        "```\n",
        "The return value should be a list or a NumPy array.\n",
        "\n",
        "***Testing.*** You can now run the tests of `encode` and `decode_predictions` below to validate your implementation, before you proceed to the next task.\n",
        "\n",
        "**5)** Implement `make_batch_tensors`.\n",
        "\n",
        "This function is an example of what is known as a *collator* in PyTorch [`DataLoader`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). The `DataLoader` (which you will try below) is a utility that divides the dataset into *batches*. A collator converts a batch into tensors that we can use when training or applying models in PyTorch.\n",
        "\n",
        "It takes a list of encoded instances in the format you created in `encode`. It then returns two PyTorch `Tensor`s, one corresponding to the documents and one to the labels.\n",
        "\n",
        "**Requirements:**\n",
        "- The output tensor corresponding to the `Y` labels should be a one-dimensional tensor (let's call its length `m`).\n",
        "- The output tensor corresponding to the `X` documents should be a two-dimensional tensor of shape `(m, n)` where `n` is the length of the longest document in this batch.\n",
        "- For documents that are shorter than `n`, you need to add the special symbol `PAD` (hard-coded to index 0) at the end so that all documents in the batch are of the same length. \n",
        "\n",
        "***Hint.*** When you pad the documents, do not *modify* the lists you created in `encode`, or you might risk a bug.\n",
        "\n",
        "***Hint.*** You can use `torch.as_tensor` to convert a regular Python list into a tensor.\n",
        "\n",
        "***Testing.*** You can now create a `DataLoader` and run the tests to make sure that you can create tensors for the batches. This batching functionality will be used when you implement the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBevnuROfTFK"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder # optional\n",
        "\n",
        "PAD = \"<PAD>\"\n",
        "UNKNOWN = \"<UNKNOWN>\"\n",
        "\n",
        "class DocumentPreprocessor:\n",
        "    def __init__(self, tokenizer, max_voc_size=None, max_len=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_voc_size = max_voc_size\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    # (1)\n",
        "    def build_vocab(self, X, Y):\n",
        "        \"\"\"\n",
        "        Build the vocabularies that will be used to encode documents and class labels.\n",
        "\n",
        "        Parameters: \n",
        "          X: a list of document strings.\n",
        "          Y: a list of document class labels.\n",
        "        \"\"\"\n",
        "        YOUR_CODE_HERE\n",
        "\n",
        "    # (2)        \n",
        "    def n_classes(self):\n",
        "        \"\"\"\n",
        "        Return the number of classes for this classification task.\n",
        "        \"\"\"\n",
        "        return YOUR_CODE_HERE\n",
        "\n",
        "    def voc_size(self):\n",
        "        \"\"\"\n",
        "        Return the number of words in the vocabulary used to encode the document.\n",
        "        \"\"\"\n",
        "        return YOUR_CODE_HERE\n",
        "        \n",
        "    # (3)        \n",
        "    def encode(self, X, Y):\n",
        "        \"\"\"        \n",
        "        Carry out integer encoding of a list of documents X and a corresponding list of labels Y.\n",
        "        \n",
        "        Parameters: \n",
        "          X: a list of document strings.\n",
        "          Y: a list of class labels.\n",
        "          \n",
        "        Returns:\n",
        "          The list of encoded instances (x, y), where each instance consists of \n",
        "          x: list of integer-encoded tokens in the document\n",
        "          y: integer-encoded class label\n",
        "        \"\"\"\n",
        "        YOUR_CODE_HERE\n",
        "        return []    \n",
        "\n",
        "    # (4)\n",
        "    def decode_predictions(self, Y):\n",
        "        \"\"\"\n",
        "        Map a sequence of integer-encoded output labels back to the original labels.\n",
        "\n",
        "        Parameters: \n",
        "          Y: a sequence of integer-encoded class labels.\n",
        "          \n",
        "        Returns:\n",
        "          The sequence of class labels in the original format.\n",
        "        \"\"\"\n",
        "        YOUR_CODE_HERE\n",
        "        return []    \n",
        "    \n",
        "    # (5)    \n",
        "    def make_batch_tensors(self, batch):\n",
        "        \"\"\"\n",
        "        Combine a list of instances into two tensors.\n",
        "        \n",
        "        Parameters:\n",
        "          batch: a list of instances (x, y), where each instance is an x-y pair as\n",
        "                 described for process_data above.\n",
        "                 \n",
        "        Returns:\n",
        "          Two PyTorch tensors Xenc, Yenc, where Xenc contains the integer-encoded documents\n",
        "          in this batch, and Yenc the integer-encoded labels.\n",
        "        \"\"\"\n",
        "        YOUR_CODE_HERE\n",
        "        \n",
        "        return torch.as_tensor([]), torch.as_tensor([])\n",
        "                    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJBKa73vUl57"
      },
      "source": [
        "### Testing your preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crqNVGYCUl58"
      },
      "source": [
        "We will now run some tests to check that your implementation seems to work correctly.\n",
        "\n",
        "We first define a preprocessor using the tokenization function we declared above. For testing purposes, we set the max vocabulary size to 256."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYz9lLHXkO8m"
      },
      "outputs": [],
      "source": [
        "testing_preprocessor = DocumentPreprocessor(tokenizer=tokenize, max_voc_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27bm7VB2Ul5_"
      },
      "source": [
        "We use the training set defined above to build the vocabularies. Make sure that the methods `build_vocab`, `n_classes`, and `voc_size` have been implemented at this point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJtPDTeXUl6A"
      },
      "outputs": [],
      "source": [
        "testing_preprocessor.build_vocab(Xtrain, Ytrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S91C4ZOJUl6B"
      },
      "source": [
        "**Testing.** The tests below check that the X and Y vocabularies have the right sizes after building the vocabularies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cII7MZNUUl6C"
      },
      "outputs": [],
      "source": [
        "# There are 2 classes in this dataset.\n",
        "assert(testing_preprocessor.n_classes() == 2)\n",
        "# The vocabulary size should be 256 as defined by our parameter.\n",
        "assert(testing_preprocessor.voc_size() == 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8_jRaaqUl6E"
      },
      "source": [
        "### Encoding the documents\n",
        "\n",
        "Now, make sure that the method `encode` (step 3) has been implemented correctly).\n",
        "\n",
        "Then let's take a few example documents and see what happens when we encode them. Make sure you understand why the output looks the way it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCacCU8jUl6E"
      },
      "outputs": [],
      "source": [
        "test_docs = ['Great idea.', 'Bowdlerized!', 'Another longer document.']\n",
        "test_labels = ['pro', 'anti', 'anti']\n",
        "\n",
        "encoded_docs = testing_preprocessor.encode(test_docs, test_labels)\n",
        "\n",
        "encoded_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqBtr9H4Ul6F"
      },
      "source": [
        "**Testing.** Now, run the tests below to check that the format of the processed documents seems OK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pCY9a-zUl6H"
      },
      "outputs": [],
      "source": [
        "# There should be 3 encoded documents.\n",
        "assert(len(encoded_docs) == 3)\n",
        "# There first document has 3 tokens and the second 2 tokens.\n",
        "assert(len(encoded_docs[0][0]) == 3)\n",
        "assert(len(encoded_docs[1][0]) == 2)\n",
        "\n",
        "# The encoded labels should be integers in [0, ..., n_classes-1].\n",
        "assert(encoded_docs[0][1] >= 0)\n",
        "assert(encoded_docs[0][1] < testing_preprocessor.n_classes())\n",
        "\n",
        "# The encoded tokens should be integers in [0, ..., voc_size-1].\n",
        "assert(all(di >= 0 and di < testing_preprocessor.voc_size() for d, _ in encoded_docs for di in d))\n",
        "\n",
        "# The first word in the second document should be out of vocabulary, encoded as 1.\n",
        "assert(encoded_docs[1][0][0] == 1)\n",
        "\n",
        "# If we decode the integer-encoded labels, we should get the original labels back.\n",
        "test_decoded = testing_preprocessor.decode_predictions([i for _, i in encoded_docs])\n",
        "assert(list(test_decoded) == test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogr7oAN3Ul6H"
      },
      "source": [
        "### Using a DataLoader\n",
        "\n",
        "As already mentioned, PyTorch provides a utility called `DataLoader` that is responsible for creating *batches* from a dataset. When implementing the training loop later, you can then easily iterate through the batches.\n",
        "\n",
        "If you want to understand more about the `DataLoader`, read [this description](https://pytorch.org/docs/stable/data.html) in the PyTorch documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDFZ2sjcUl6I"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhQg82TqUl6I"
      },
      "source": [
        "We now create a `DataLoader`. It operates on top of a dataset: in our case, the list of encoded instances. In this example, we set the batch size to 2 and we tell the `DataLoader` to process the instances in order without shuffling.\n",
        "\n",
        "We also need to provide the collator `make_batch_tensors` we defined above. As you know, it takes a batch and creates tensors that we can use with a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxfNu-UIUl6J"
      },
      "outputs": [],
      "source": [
        "dl = DataLoader(encoded_docs, 2, shuffle=False, collate_fn=testing_preprocessor.make_batch_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULrAg84qUl6K"
      },
      "source": [
        "This object now acts as any Python iterable. When iterating over this object, we go through all the batches. (If you set `shuffle` to `True`, the order of the instances will be randomized each time you restart the iteration.)\n",
        "\n",
        "Finally, let's run some tests to make sure that your collator is implemented correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlEoVnRZUl6K"
      },
      "outputs": [],
      "source": [
        "for i, (Xbatch, Ybatch) in enumerate(dl):\n",
        "    \n",
        "    # There should be 2 batches since there are 3 instances and we set the batch size to 2.\n",
        "    assert(i < 2)\n",
        "\n",
        "    # The returned values should be tensors.\n",
        "    assert(isinstance(Xbatch, torch.Tensor))\n",
        "    assert(isinstance(Ybatch, torch.Tensor))\n",
        "    \n",
        "    if i == 0:\n",
        "        # We set the batch size to 2. The longest document in the first batch has length 3.\n",
        "        assert(Xbatch.shape == (2, 3))\n",
        "        assert(Ybatch.shape == (2,))\n",
        "\n",
        "        # The first token in the second document is out of vocabulary (1).\n",
        "        assert(Xbatch[1, 0] == 1)\n",
        "\n",
        "        # The last token in the second document is padding (0).\n",
        "        assert(Xbatch[1, 2] == 0)        \n",
        "    else:\n",
        "        # One document in the last batch. It has length 4.\n",
        "        assert(Xbatch.shape == (1, 4))\n",
        "        assert(Ybatch.shape == (1,))\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss7HNr79Ul6L"
      },
      "source": [
        "# Part 2: Implementing the training loop\n",
        "\n",
        "We will now write the code that runs the training loop to train the parameters of a neural network model. This implementation is agnostic with respect to the network structure and we will define the actual model elsewhere.\n",
        "\n",
        "**Your work.** Fill in the missing parts of this code, labeled as `YOUR_CODE_HERE`.\n",
        "\n",
        "In `train_model`, the main part of the training loop, you only need to define the loss function and the optimizer.\n",
        "\n",
        "**Hint.** You may assume that there is an arbitrary number of classes, even though in this example we know that there are 2 classes. So you should use a multiclass loss, not a binary loss. (Our implementation below is also based on the assumption of a multiclass structure.)\n",
        "\n",
        "Most of your work will be done in the function `apply_model`. This function takes a data loader, goes through the batches, and applies the model to each batch. If we are training the model (that is, if an optimizer was provided), we update the model after each batch. Here, you will need to carry out the typical steps in a PyTorch training loop: get the batch tensors from the `DataLoader`, put the tensors on the GPU (if you are using one), apply the model, compute the loss, and update the model. We will also collect some statistics along the way.\n",
        "\n",
        "We will not be able to test your implementation until we run it on the actual data and model, which we will do in part 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARmRzaLAUl6M"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def train_model(model, data_train, data_val, par):\n",
        "    \"\"\"Train the model on the given training data.\n",
        "\n",
        "    Parameters:\n",
        "      model:      the PyTorch model that will be trained.\n",
        "      data_train: the DataLoader that generates the input-output batches for training.\n",
        "      data_val:   the DataLoader for validataion.\n",
        "      par:        an object containing all relevant hyperparameters.\n",
        "\n",
        "    Returns:\n",
        "      history:    a dict containing statistics computed over the epochs.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define a loss function that is suitable for a multiclass classification task.\n",
        "    loss_func = YOUR_CODE_HERE\n",
        "    \n",
        "    # Define an optimizer that will update the model's parameters.\n",
        "    # You can assume that `par` contains the hyperparameters you need here.\n",
        "    optimizer = YOUR_CODE_HERE\n",
        "\n",
        "    # Contains the statistics that will be returned.\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    progress = tqdm(range(par.n_epochs), 'Epochs')        \n",
        "    for epoch in progress:\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Put the model in \"training mode\". Will affect e.g. dropout, batch normalizers.\n",
        "        model.train()\n",
        "        \n",
        "        # Run the model on the training set, update the model, and get the training set statistics.\n",
        "        train_loss, train_acc = apply_model(model, data_train, loss_func, optimizer)\n",
        "\n",
        "        # Put the model in \"evaluation mode\". Will affect e.g. dropout, batch normalizers.        \n",
        "        model.eval()\n",
        "        \n",
        "        # Turn off gradient computation, since we are not updating the model now.\n",
        "        with torch.no_grad():\n",
        "            # Run the model on the validation set and get the training set statistics.\n",
        "            val_loss, val_acc = apply_model(model, data_val, loss_func)\n",
        "\n",
        "        t1 = time.time()\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['time'].append(t1-t0)\n",
        "        \n",
        "        progress.set_postfix({'val_loss': f'{val_loss:.2f}', 'val_acc': f'{val_acc:.2f}'})\n",
        "    \n",
        "    return history\n",
        "        \n",
        "def apply_model(model, data, loss_func, optimizer=None):\n",
        "    \"\"\"Run the neural network for one epoch, using the given batches.\n",
        "    If an optimizer is provided, this is training data and we will update the model\n",
        "    after each batch. Otherwise, this is assumed to be validation data.\n",
        "\n",
        "    Parameters:\n",
        "      model:     the PyTorch model.\n",
        "      data:      the DataLoader that generates the input-output batches.\n",
        "      loss_func: the loss function\n",
        "      optimizer: the optimizer; should be None if we are running on validation data.\n",
        "\n",
        "    Returns the loss and accuracy over the epoch.\"\"\"\n",
        "    n_correct = 0\n",
        "    n_instances = 0\n",
        "    total_loss = 0\n",
        "    \n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # for each X, Y pair in the batch:    \n",
        "    for Xbatch, Ybatch in YOUR_CODE_HERE:\n",
        "            \n",
        "        # put X and Y on the device\n",
        "        Xbatch = YOUR_CODE_HERE\n",
        "        Ybatch = YOUR_CODE_HERE\n",
        "         \n",
        "        assert(isinstance(Xbatch, torch.Tensor))\n",
        "        assert(isinstance(Ybatch, torch.Tensor))   \n",
        "            \n",
        "        # forward pass part 1: apply the model on X to get \n",
        "        # the model's outputs for this batch\n",
        "        model_output = YOUR_CODE_HERE\n",
        "\n",
        "        assert(len(model_output.shape) == 2)\n",
        "        assert(model_output.shape[0] == Ybatch.shape[0])\n",
        "        \n",
        "        # forward pass part 2: compute the loss by comparing\n",
        "        # the model output to the reference Y values\n",
        "        loss = YOUR_CODE_HERE\n",
        "        \n",
        "        assert(not loss.shape)\n",
        "        \n",
        "        # update the loss statistics\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # convert the scores computed above into hard decisions\n",
        "        guesses = model_output.argmax(dim=1)\n",
        "        \n",
        "        # compute the number of correct predictions and update the statistics\n",
        "        n_correct += (guesses == Ybatch).sum().item()\n",
        "        n_instances += Ybatch.shape[0]\n",
        "\n",
        "        # if we have an optimizer, it means we are processing the training set\n",
        "        # so that the model needs to be updated after each batch\n",
        "        if optimizer:\n",
        "            \n",
        "            # reset the gradients\n",
        "            YOUR_CODE_HERE\n",
        "            \n",
        "            # backprop to compute the new gradients\n",
        "            YOUR_CODE_HERE\n",
        "            \n",
        "            # use the optimizer to update the model\n",
        "            YOUR_CODE_HERE\n",
        "            \n",
        "    return total_loss/len(data), n_correct/n_instances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FPvceV5Ul6O"
      },
      "source": [
        "# Part 3: Training a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u51L7TwoUl6P"
      },
      "source": [
        "The following code builds the neural network structure.\n",
        "\n",
        "The details will be explained in Lecture 2. It builds a simple type of neural network that we can use to classify documents. It computes *word embeddings* for each word in the document, and then represents a document as a mean over the word embeddings. Finally, a linear layer is put on top of the document representations to compute the final prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDCYL5LsUl6R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class CBoWRepresentation(nn.Module):\n",
        "    \n",
        "    def __init__(self, voc_size, emb_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Initialize the parameters. The only parameters of this representation model are the word embeddings.\n",
        "        self.embedding = nn.Embedding(voc_size, emb_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X is a batch tensor with shape (batch_size, max_doc_length). \n",
        "        # Each row contains integer-encoded words.\n",
        "        \n",
        "        # Look up the word embeddings for the words in the documents.\n",
        "        # The result should have the shape (batch_size, max_doc_length, emb_dim)\n",
        "        embedded = self.embedding(X)\n",
        "               \n",
        "        # Compute a mask that hides the padding tokens. We hard-code the padding index 0 here.\n",
        "        mask = X != 0\n",
        "        \n",
        "        # Sum the embeddings for the non-masked positions.\n",
        "        summed = (embedded.permute((2, 0, 1))*mask).sum(dim=2).t()\n",
        "        \n",
        "        # Denominators when computing the means.\n",
        "        n_not_masked = mask.sum(dim=1, keepdim=True)\n",
        "\n",
        "        # Compute the means.\n",
        "        means = summed / n_not_masked\n",
        "        \n",
        "        # The result should be a tensor of shape (batch_size, emb_dim)\n",
        "        return means\n",
        "    \n",
        "def make_cbow_nn(preprocessor, params):\n",
        "    # Use a Sequential to build a stacked neural network.\n",
        "    # We combine the document representation component with a linear output layer.\n",
        "    return nn.Sequential(\n",
        "            CBoWRepresentation(preprocessor.voc_size(), params.emb_dim),\n",
        "            nn.Linear(in_features=params.emb_dim, out_features=preprocessor.n_classes())            \n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94YTNHaAUl6S"
      },
      "source": [
        "The following container is used to collect various hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgDK_CagUl6T"
      },
      "outputs": [],
      "source": [
        "class CBoWParameters:\n",
        "    \"\"\"Container class to store the hyperparameters that control the training process.\"\"\"\n",
        "\n",
        "    # Dimensionality of word embeddings.\n",
        "    emb_dim = 32\n",
        "\n",
        "    # Learning rate for the optimizer. You might need to change it, depending on which optimizer you use.\n",
        "    learning_rate = 3e-3\n",
        "\n",
        "    # Number of training epochs (passes through the training set).\n",
        "    n_epochs = 20\n",
        "    \n",
        "    # Batch size used by the data loaders.\n",
        "    batch_size = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXW3oyBcUl6V"
      },
      "source": [
        "Now, combine all the pieces we have created above.\n",
        "\n",
        "- Preprocess the training and validation sets and create corresponding data loaders.\n",
        "- Create a model.\n",
        "- Run the training loop.\n",
        "\n",
        "If your code works, you should see a progress bar advancing after each epoch. The progress bar displays the loss and accuracy scores computed on the validation set after each epoch.\n",
        "\n",
        "You may get different results depending on your implementation as well as random factors due to initialization. A reasonable implementation will typically see accuracies in the range 0.70-0.80 after training for some epochs. If the accuracies are lower or higher than that, you probably have a bug somewhere.\n",
        "\n",
        "**Optionally**, you may apply other models discussed in Lecture 2 and see how well they work for this classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMAXvMOUUl6V"
      },
      "source": [
        "# Part 4: Applying the trained model to new instances\n",
        "\n",
        "It is now a simple matter to write a function that takes the trained model and applies it to a dataset to classify all instances. Implement this function: this will be a bit similar to `apply_model` above, except that we also have to return the predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6PDdWbVUl6W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict(model, data):   \n",
        "    device = next(model.parameters()).device\n",
        "    outputs = []    \n",
        "    for Xbatch, _ in YOUR_CODE_HERE:\n",
        "        Xbatch = YOUR_CODE_HERE\n",
        "        scores = YOUR_CODE_HERE\n",
        "        predictions = YOUR_CODE_HERE        \n",
        "        outputs.append(predictions.detach().cpu().numpy())\n",
        "    return np.stack(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9xWdyUYUl6X"
      },
      "source": [
        "Try to invent a few examples and see if your model classifies them correctly. Recall that you implemented a method `decode_predictions` that maps the integer-encoded `Y` values back to the string labels.\n",
        "\n",
        "Finally, load the test data from the file `brexit_test.csv`, classify the instances, and compute the accuracy of the predictions (e.g. using [`sklearn.metrics.accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) or directly)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}